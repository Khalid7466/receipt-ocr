{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca171781",
   "metadata": {},
   "source": [
    "# OCR Pipeline for Receipt Text Extraction\n",
    "\n",
    "This notebook implements the OCR (Optical Character Recognition) step of our receipt processing pipeline.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Detection** (completed) → YOLO detects text regions on receipts\n",
    "2. **OCR** (this notebook) → Extract text from detected regions\n",
    "3. **Parsing** → Structure the extracted text into key-value pairs\n",
    "\n",
    "## OCR Engines Compared\n",
    "| Engine | Accuracy | Speed | Languages | Best For |\n",
    "|--------|----------|-------|-----------|----------|\n",
    "| **EasyOCR** | High | Medium | 80+ | General purpose, receipts |\n",
    "| **PaddleOCR** | Very High | Fast | 80+ | Chinese/Asian text |\n",
    "| **Tesseract** | Medium | Fast | 100+ | Simple documents |\n",
    "| **TrOCR** | Very High | Slow | English | Printed text |\n",
    "\n",
    "We'll use **EasyOCR** as our primary engine due to its excellent performance on receipt images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91644013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell once to install dependencies\n",
    "%pip install easyocr paddleocr pytesseract opencv-python-headless pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab9c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print(\"✓ Base libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ded8be",
   "metadata": {},
   "source": [
    "## Step 1: Initialize OCR Engines\n",
    "\n",
    "We'll set up EasyOCR as our primary engine. It provides excellent accuracy for receipt text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EasyOCR\n",
    "import easyocr\n",
    "\n",
    "# Create EasyOCR reader (downloads models on first run)\n",
    "# Languages: 'en' for English, add more as needed e.g., ['en', 'th'] for Thai\n",
    "print(\"Initializing EasyOCR (this may take a moment on first run)...\")\n",
    "reader = easyocr.Reader(\n",
    "    ['en'],  # Languages to support\n",
    "    gpu=True,  # Use GPU if available\n",
    "    model_storage_directory='models/easyocr',\n",
    "    download_enabled=True\n",
    ")\n",
    "print(\"✓ EasyOCR initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc62a",
   "metadata": {},
   "source": [
    "## Step 2: Define OCR Engine Class\n",
    "\n",
    "A unified interface for different OCR backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b619a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceiptOCR:\n",
    "    \"\"\"\n",
    "    Unified OCR interface for receipt text extraction.\n",
    "    Supports EasyOCR with preprocessing optimizations for receipts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reader: easyocr.Reader):\n",
    "        self.reader = reader\n",
    "    \n",
    "    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess image for better OCR accuracy on receipts.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR format from cv2)\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed grayscale image\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image.copy()\n",
    "        \n",
    "        # Apply adaptive thresholding for better contrast\n",
    "        # This helps with faded receipt text\n",
    "        denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "        \n",
    "        # Adaptive threshold works well for receipts with varying lighting\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            denoised, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,\n",
    "            blockSize=11,\n",
    "            C=2\n",
    "        )\n",
    "        \n",
    "        return binary\n",
    "    \n",
    "    def extract_text(\n",
    "        self, \n",
    "        image: np.ndarray,\n",
    "        preprocess: bool = True,\n",
    "        detail: int = 1,\n",
    "        paragraph: bool = False\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text from an image using EasyOCR.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR or grayscale)\n",
    "            preprocess: Whether to apply preprocessing\n",
    "            detail: 0 for simple output, 1 for detailed (with bbox & confidence)\n",
    "            paragraph: Whether to merge text into paragraphs\n",
    "            \n",
    "        Returns:\n",
    "            List of detected text with bounding boxes and confidence scores\n",
    "        \"\"\"\n",
    "        if preprocess:\n",
    "            processed = self.preprocess_image(image)\n",
    "        else:\n",
    "            processed = image\n",
    "        \n",
    "        # Run OCR\n",
    "        results = self.reader.readtext(\n",
    "            processed,\n",
    "            detail=detail,\n",
    "            paragraph=paragraph,\n",
    "            min_size=10,\n",
    "            text_threshold=0.7,\n",
    "            low_text=0.4,\n",
    "            link_threshold=0.4,\n",
    "            canvas_size=2560,\n",
    "            mag_ratio=1.5\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            if detail == 1:\n",
    "                bbox, text, confidence = result\n",
    "                formatted_results.append({\n",
    "                    'bbox': bbox,\n",
    "                    'text': text,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "            else:\n",
    "                formatted_results.append({'text': result})\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def extract_from_regions(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        regions: List[Tuple[int, int, int, int]],\n",
    "        padding: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text from specific regions (e.g., YOLO detections).\n",
    "        \n",
    "        Args:\n",
    "            image: Full image\n",
    "            regions: List of (x1, y1, x2, y2) bounding boxes\n",
    "            padding: Pixels to add around each region\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted text for each region\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        for i, (x1, y1, x2, y2) in enumerate(regions):\n",
    "            # Add padding and clip to image bounds\n",
    "            x1 = max(0, x1 - padding)\n",
    "            y1 = max(0, y1 - padding)\n",
    "            x2 = min(w, x2 + padding)\n",
    "            y2 = min(h, y2 + padding)\n",
    "            \n",
    "            # Crop region\n",
    "            region_img = image[y1:y2, x1:x2]\n",
    "            \n",
    "            # Skip if region is too small\n",
    "            if region_img.shape[0] < 10 or region_img.shape[1] < 10:\n",
    "                continue\n",
    "            \n",
    "            # Extract text\n",
    "            text_results = self.extract_text(region_img, preprocess=True)\n",
    "            \n",
    "            # Combine text from region\n",
    "            combined_text = ' '.join([r['text'] for r in text_results])\n",
    "            avg_confidence = np.mean([r['confidence'] for r in text_results]) if text_results else 0\n",
    "            \n",
    "            results.append({\n",
    "                'region_id': i,\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'text': combined_text,\n",
    "                'confidence': avg_confidence,\n",
    "                'details': text_results\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize our OCR engine\n",
    "ocr_engine = ReceiptOCR(reader)\n",
    "print(\"✓ ReceiptOCR engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0120246",
   "metadata": {},
   "source": [
    "## Step 3: Test OCR on Sample Receipt Images\n",
    "\n",
    "Let's test our OCR on some receipt images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ade4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sample receipt images\n",
    "cord_test_dir = Path('data/cord/raw/test')\n",
    "sroie_test_dir = Path('data/SROIE2019/test/img')\n",
    "\n",
    "# Get sample images\n",
    "sample_images = []\n",
    "if cord_test_dir.exists():\n",
    "    sample_images.extend(list(cord_test_dir.glob('*.png'))[:3])\n",
    "if sroie_test_dir.exists():\n",
    "    sample_images.extend(list(sroie_test_dir.glob('*.jpg'))[:3])\n",
    "\n",
    "print(f\"Found {len(sample_images)} sample images for testing\")\n",
    "for img_path in sample_images:\n",
    "    print(f\"  - {img_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a088b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OCR on a single image\n",
    "def visualize_ocr_results(image_path: Path, ocr_engine: ReceiptOCR):\n",
    "    \"\"\"Visualize OCR results on an image.\"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Run OCR\n",
    "    start_time = time.time()\n",
    "    results = ocr_engine.extract_text(image, preprocess=False, detail=1)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Draw results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image_rgb)\n",
    "    axes[0].set_title(f\"Original: {image_path.name}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Image with OCR annotations\n",
    "    annotated = image_rgb.copy()\n",
    "    for result in results:\n",
    "        bbox = result['bbox']\n",
    "        text = result['text']\n",
    "        conf = result['confidence']\n",
    "        \n",
    "        # Convert bbox to integer points\n",
    "        pts = np.array(bbox, dtype=np.int32)\n",
    "        \n",
    "        # Draw polygon\n",
    "        cv2.polylines(annotated, [pts], True, (0, 255, 0), 2)\n",
    "        \n",
    "        # Add text label\n",
    "        x, y = int(pts[0][0]), int(pts[0][1]) - 5\n",
    "        cv2.putText(annotated, f\"{text[:20]}...\", (x, y), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "    \n",
    "    axes[1].imshow(annotated)\n",
    "    axes[1].set_title(f\"OCR Results ({len(results)} detections, {elapsed:.2f}s)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print extracted text\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Extracted Text from {image_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1:3}. [{result['confidence']:.2f}] {result['text']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on first sample image\n",
    "if sample_images:\n",
    "    results = visualize_ocr_results(sample_images[0], ocr_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3779db",
   "metadata": {},
   "source": [
    "## Step 4: Integrate with YOLO Detection\n",
    "\n",
    "Now let's combine our trained YOLO detector with the OCR engine for a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c51b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained YOLO model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Try to load the best combined model, fall back to CORD model\n",
    "model_paths = [\n",
    "    'runs/yolo11n_combined/train/weights/best.pt',\n",
    "    'runs/yolo11n_cord/detect/weights/best.pt',\n",
    "    'yolo11n.pt'  # Fallback to pretrained\n",
    "]\n",
    "\n",
    "yolo_model = None\n",
    "for model_path in model_paths:\n",
    "    if Path(model_path).exists():\n",
    "        print(f\"Loading YOLO model from: {model_path}\")\n",
    "        yolo_model = YOLO(model_path)\n",
    "        print(\"✓ YOLO model loaded successfully!\")\n",
    "        break\n",
    "\n",
    "if yolo_model is None:\n",
    "    print(\"⚠ No trained model found. Using pretrained yolo11n.pt\")\n",
    "    yolo_model = YOLO('yolo11n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceiptPipeline:\n",
    "    \"\"\"\n",
    "    Complete receipt processing pipeline:\n",
    "    1. YOLO detection for text regions\n",
    "    2. OCR for text extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, yolo_model: YOLO, ocr_engine: ReceiptOCR):\n",
    "        self.detector = yolo_model\n",
    "        self.ocr = ocr_engine\n",
    "    \n",
    "    def detect_regions(\n",
    "        self, \n",
    "        image: np.ndarray, \n",
    "        conf_threshold: float = 0.25\n",
    "    ) -> List[Tuple[int, int, int, int]]:\n",
    "        \"\"\"\n",
    "        Detect text regions using YOLO.\n",
    "        \n",
    "        Returns:\n",
    "            List of (x1, y1, x2, y2) bounding boxes\n",
    "        \"\"\"\n",
    "        results = self.detector.predict(\n",
    "            image, \n",
    "            conf=conf_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        regions = []\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                for box in result.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                    conf = box.conf[0].cpu().numpy()\n",
    "                    regions.append((x1, y1, x2, y2, conf))\n",
    "        \n",
    "        # Sort by y-coordinate (top to bottom), then x (left to right)\n",
    "        regions.sort(key=lambda r: (r[1], r[0]))\n",
    "        \n",
    "        return [(r[0], r[1], r[2], r[3]) for r in regions]\n",
    "    \n",
    "    def process_image(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        conf_threshold: float = 0.25,\n",
    "        use_detection: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a receipt image through the full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the receipt image\n",
    "            conf_threshold: Confidence threshold for detection\n",
    "            use_detection: If True, use YOLO detection first; if False, OCR whole image\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detected regions and extracted text\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        \n",
    "        result = {\n",
    "            'image_path': str(image_path),\n",
    "            'image_size': (image.shape[1], image.shape[0]),\n",
    "            'regions': [],\n",
    "            'full_text': ''\n",
    "        }\n",
    "        \n",
    "        if use_detection:\n",
    "            # Step 1: Detect text regions\n",
    "            regions = self.detect_regions(image, conf_threshold)\n",
    "            result['num_regions'] = len(regions)\n",
    "            \n",
    "            # Step 2: Extract text from each region\n",
    "            for i, (x1, y1, x2, y2) in enumerate(regions):\n",
    "                region_img = image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Skip tiny regions\n",
    "                if region_img.shape[0] < 10 or region_img.shape[1] < 10:\n",
    "                    continue\n",
    "                \n",
    "                # OCR on region\n",
    "                ocr_results = self.ocr.extract_text(region_img, preprocess=True)\n",
    "                text = ' '.join([r['text'] for r in ocr_results])\n",
    "                avg_conf = np.mean([r['confidence'] for r in ocr_results]) if ocr_results else 0\n",
    "                \n",
    "                result['regions'].append({\n",
    "                    'id': i,\n",
    "                    'bbox': (x1, y1, x2, y2),\n",
    "                    'text': text,\n",
    "                    'confidence': float(avg_conf)\n",
    "                })\n",
    "        else:\n",
    "            # OCR on full image\n",
    "            ocr_results = self.ocr.extract_text(image, preprocess=True)\n",
    "            for i, r in enumerate(ocr_results):\n",
    "                result['regions'].append({\n",
    "                    'id': i,\n",
    "                    'bbox': r['bbox'],\n",
    "                    'text': r['text'],\n",
    "                    'confidence': r['confidence']\n",
    "                })\n",
    "        \n",
    "        # Combine all text\n",
    "        result['full_text'] = '\\n'.join([r['text'] for r in result['regions'] if r['text']])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_results(self, image_path: str, result: Dict):\n",
    "        \"\"\"Visualize pipeline results.\"\"\"\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "        \n",
    "        # Draw detections\n",
    "        annotated = image_rgb.copy()\n",
    "        for region in result['regions']:\n",
    "            x1, y1, x2, y2 = region['bbox'][:4] if len(region['bbox']) == 4 else (\n",
    "                int(min(p[0] for p in region['bbox'])),\n",
    "                int(min(p[1] for p in region['bbox'])),\n",
    "                int(max(p[0] for p in region['bbox'])),\n",
    "                int(max(p[1] for p in region['bbox']))\n",
    "            )\n",
    "            \n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Add text preview\n",
    "            text_preview = region['text'][:15] + '...' if len(region['text']) > 15 else region['text']\n",
    "            cv2.putText(annotated, text_preview, (x1, y1-5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "        \n",
    "        axes[0].imshow(annotated)\n",
    "        axes[0].set_title(f\"Detection + OCR ({len(result['regions'])} regions)\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Show extracted text\n",
    "        axes[1].text(0.05, 0.95, result['full_text'], \n",
    "                    transform=axes[1].transAxes,\n",
    "                    fontsize=9, verticalalignment='top',\n",
    "                    fontfamily='monospace',\n",
    "                    wrap=True)\n",
    "        axes[1].set_title(\"Extracted Text\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ReceiptPipeline(yolo_model, ocr_engine)\n",
    "print(\"✓ Receipt processing pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebb2ae",
   "metadata": {},
   "source": [
    "## Step 5: Run Full Pipeline on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sample images through the full pipeline\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Sample Receipts Through Full Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, img_path in enumerate(sample_images[:3]):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Image {i+1}: {img_path.name}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Run pipeline\n",
    "        start_time = time.time()\n",
    "        result = pipeline.process_image(img_path, conf_threshold=0.25, use_detection=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"Processing time: {elapsed:.2f}s\")\n",
    "        print(f\"Regions detected: {len(result['regions'])}\")\n",
    "        \n",
    "        # Visualize\n",
    "        pipeline.visualize_results(img_path, result)\n",
    "        \n",
    "        # Print extracted text\n",
    "        print(f\"\\nExtracted Text:\")\n",
    "        print(result['full_text'][:500] + \"...\" if len(result['full_text']) > 500 else result['full_text'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bb700",
   "metadata": {},
   "source": [
    "## Step 6: Batch Processing\n",
    "\n",
    "Process multiple receipts and export results to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48174e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_process_receipts(\n",
    "    image_dir: Path,\n",
    "    output_path: Path,\n",
    "    pipeline: ReceiptPipeline,\n",
    "    max_images: int = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Batch process multiple receipt images.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing receipt images\n",
    "        output_path: Path to save JSON results\n",
    "        pipeline: ReceiptPipeline instance\n",
    "        max_images: Maximum number of images to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        List of processing results\n",
    "    \"\"\"\n",
    "    # Find all images\n",
    "    image_files = list(image_dir.glob('*.png')) + list(image_dir.glob('*.jpg'))\n",
    "    if max_images:\n",
    "        image_files = image_files[:max_images]\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images from {image_dir}\")\n",
    "    \n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Processing\"):\n",
    "        try:\n",
    "            result = pipeline.process_image(img_path, use_detection=False)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            errors.append({'image': str(img_path), 'error': str(e)})\n",
    "    \n",
    "    # Save results\n",
    "    output_data = {\n",
    "        'processed': len(results),\n",
    "        'errors': len(errors),\n",
    "        'results': results,\n",
    "        'error_details': errors\n",
    "    }\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to {output_path}\")\n",
    "    print(f\"  Processed: {len(results)}\")\n",
    "    print(f\"  Errors: {len(errors)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process CORD test images\n",
    "output_dir = Path('outputs/ocr_results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if cord_test_dir.exists():\n",
    "    cord_results = batch_process_receipts(\n",
    "        cord_test_dir,\n",
    "        output_dir / 'cord_ocr_results.json',\n",
    "        pipeline,\n",
    "        max_images=10  # Limit for demo\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10eaa80",
   "metadata": {},
   "source": [
    "## Step 7: Save OCR Engine to Module\n",
    "\n",
    "Let's save our OCR classes to the src/ocr module for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef935caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the OCR engine code to src/ocr/engine.py\n",
    "ocr_engine_code = '''\"\"\"\n",
    "Receipt OCR Engine using EasyOCR.\n",
    "\n",
    "This module provides OCR functionality optimized for receipt images.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ReceiptOCR:\n",
    "    \"\"\"\n",
    "    OCR engine optimized for receipt text extraction.\n",
    "    Uses EasyOCR with preprocessing for better accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        languages: List[str] = ['en'],\n",
    "        gpu: bool = True,\n",
    "        model_storage_directory: str = 'models/easyocr'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OCR engine.\n",
    "        \n",
    "        Args:\n",
    "            languages: List of language codes to support\n",
    "            gpu: Whether to use GPU acceleration\n",
    "            model_storage_directory: Directory to store OCR models\n",
    "        \"\"\"\n",
    "        self.reader = easyocr.Reader(\n",
    "            languages,\n",
    "            gpu=gpu,\n",
    "            model_storage_directory=model_storage_directory,\n",
    "            download_enabled=True\n",
    "        )\n",
    "        self.languages = languages\n",
    "    \n",
    "    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess image for better OCR accuracy on receipts.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR format from cv2)\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed grayscale image\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image.copy()\n",
    "        \n",
    "        # Denoise\n",
    "        denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "        \n",
    "        # Adaptive threshold for receipts with varying lighting\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            denoised, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,\n",
    "            blockSize=11,\n",
    "            C=2\n",
    "        )\n",
    "        \n",
    "        return binary\n",
    "    \n",
    "    def extract_text(\n",
    "        self, \n",
    "        image: np.ndarray,\n",
    "        preprocess: bool = True,\n",
    "        detail: int = 1,\n",
    "        paragraph: bool = False,\n",
    "        min_size: int = 10,\n",
    "        text_threshold: float = 0.7,\n",
    "        low_text: float = 0.4,\n",
    "        link_threshold: float = 0.4\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR or grayscale)\n",
    "            preprocess: Whether to apply preprocessing\n",
    "            detail: 0 for text only, 1 for detailed output with bbox & confidence\n",
    "            paragraph: Whether to merge text into paragraphs\n",
    "            min_size: Minimum text size to detect\n",
    "            text_threshold: Text confidence threshold\n",
    "            low_text: Low text bound\n",
    "            link_threshold: Link threshold for connecting text\n",
    "            \n",
    "        Returns:\n",
    "            List of detected text with bounding boxes and confidence scores\n",
    "        \"\"\"\n",
    "        if preprocess:\n",
    "            processed = self.preprocess_image(image)\n",
    "        else:\n",
    "            processed = image\n",
    "        \n",
    "        results = self.reader.readtext(\n",
    "            processed,\n",
    "            detail=detail,\n",
    "            paragraph=paragraph,\n",
    "            min_size=min_size,\n",
    "            text_threshold=text_threshold,\n",
    "            low_text=low_text,\n",
    "            link_threshold=link_threshold,\n",
    "            canvas_size=2560,\n",
    "            mag_ratio=1.5\n",
    "        )\n",
    "        \n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            if detail == 1:\n",
    "                bbox, text, confidence = result\n",
    "                formatted_results.append({\n",
    "                    \\'bbox\\': bbox,\n",
    "                    \\'text\\': text,\n",
    "                    \\'confidence\\': confidence\n",
    "                })\n",
    "            else:\n",
    "                formatted_results.append({\\'text\\': result})\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def extract_from_regions(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        regions: List[Tuple[int, int, int, int]],\n",
    "        padding: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text from specific regions.\n",
    "        \n",
    "        Args:\n",
    "            image: Full image\n",
    "            regions: List of (x1, y1, x2, y2) bounding boxes\n",
    "            padding: Pixels to add around each region\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted text for each region\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        for i, (x1, y1, x2, y2) in enumerate(regions):\n",
    "            # Add padding and clip\n",
    "            x1 = max(0, x1 - padding)\n",
    "            y1 = max(0, y1 - padding)\n",
    "            x2 = min(w, x2 + padding)\n",
    "            y2 = min(h, y2 + padding)\n",
    "            \n",
    "            # Crop region\n",
    "            region_img = image[y1:y2, x1:x2]\n",
    "            \n",
    "            if region_img.shape[0] < 10 or region_img.shape[1] < 10:\n",
    "                continue\n",
    "            \n",
    "            text_results = self.extract_text(region_img, preprocess=True)\n",
    "            combined_text = \\' \\'.join([r[\\'text\\'] for r in text_results])\n",
    "            avg_confidence = np.mean([r[\\'confidence\\'] for r in text_results]) if text_results else 0\n",
    "            \n",
    "            results.append({\n",
    "                \\'region_id\\': i,\n",
    "                \\'bbox\\': (x1, y1, x2, y2),\n",
    "                \\'text\\': combined_text,\n",
    "                \\'confidence\\': avg_confidence,\n",
    "                \\'details\\': text_results\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "engine_path = Path('src/ocr/engine.py')\n",
    "engine_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(engine_path, 'w') as f:\n",
    "    f.write(ocr_engine_code)\n",
    "\n",
    "print(f\"✓ OCR engine saved to {engine_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5cd6de",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete OCR pipeline for receipt text extraction:\n",
    "\n",
    "### Components Created:\n",
    "1. **ReceiptOCR** - EasyOCR wrapper with receipt-specific preprocessing\n",
    "2. **ReceiptPipeline** - Combines YOLO detection + OCR extraction\n",
    "3. **Batch Processing** - Process multiple receipts and export to JSON\n",
    "\n",
    "### Key Features:\n",
    "- Adaptive thresholding for faded receipt text\n",
    "- Denoising for cleaner OCR input\n",
    "- Support for region-based extraction (using YOLO detections)\n",
    "- Full-image OCR fallback\n",
    "- JSON export for downstream processing\n",
    "\n",
    "### Next Steps:\n",
    "- Notebook `03_parsing.ipynb`: Parse extracted text into structured key-value pairs\n",
    "- Notebook `04_kie_evaluation.ipynb`: Evaluate Key Information Extraction accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
